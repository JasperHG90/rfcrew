# RFC: BigQuery Ingestion via Storage Write API (Pending Stream) from Cloud Run

## üìú Table of contents
---
```table-of-contents```

## ü§ì TL;DR;
---
This RFC proposes using the BigQuery Storage Write API's `Pending` stream type with Protocol Buffers (Protobuf) for batch ingesting data every 10 minutes from a Python application running on Cloud Run into BigQuery. This approach provides atomic commits for batches, efficient data transfer via gRPC, schema evolution detection capabilities, and lower costs compared to the legacy BigQuery streaming API, while adhering to the specific constraint of not using Google Cloud Pub/Sub.

## üî≠ Context and Scope
---
**Background:**
Our team operates a Python application deployed on Google Cloud Run. This application generates data that needs to be reliably ingested into a BigQuery table for downstream analysis and reporting. Currently, there is no standardized or optimized mechanism in place for this specific ingestion task.

**Problem Definition:**
We require an efficient, reliable, and cost-effective method to ingest batches of data generated by the Cloud Run Python application into BigQuery approximately every 10 minutes. The solution must handle potential schema evolution in the source data and ensure data integrity (atomicity per batch) during ingestion. A specific constraint is that Google Cloud Pub/Sub cannot be used as part of this ingestion pipeline.

**Scope:**
*   **In Scope:**
    *   Defining the mechanism for batch data ingestion from the Python Cloud Run application to BigQuery.
    *   Utilizing the BigQuery Storage Write API.
    *   Using Protocol Buffers (Protobuf) as the data serialization format.
    *   Handling data batches generated roughly every 10 minutes.
    *   Addressing schema evolution detection at the client level and defining the initial handling strategy (alerting/manual intervention).
    *   Specifying the appropriate Storage Write API stream type (`Pending` type) for atomic batch commits.
    *   Using the official Google Cloud Python client library (`google-cloud-bigquery-storage`).
    *   Defining necessary IAM permissions and security considerations.
    *   Identifying key monitoring metrics and operational requirements.
*   **Out of Scope:**
    *   The specific implementation details of the data generation logic within the Python application.
    *   Detailed implementation of downstream data processing or analysis in BigQuery.
    *   Detailed infrastructure setup for Cloud Run or BigQuery beyond the API interaction and IAM configuration (e.g., specific network configurations unless security requires it).
    *   Real-time (sub-minute latency) streaming requirements.
    *   Using alternative ingestion methods like BigQuery Load Jobs via GCS (though considered as an alternative).
    *   Using Google Cloud Pub/Sub for buffering or transport.
    *   Implementation of a fully automated schema migration process.
    *   Detailed implementation of a dead-letter queue (though the need is discussed).

## üéØ Goals (and Non-Goals)
---
*   **Goals:**
    *   Implement reliable batch ingestion of data from the Cloud Run Python app to BigQuery with atomicity for each batch.
    *   Achieve efficient data transfer using Protobuf and gRPC via the Storage Write API.
    *   Ensure atomicity for each ~10-minute batch using transactional commits (`Pending` stream type).
    *   Support detection of BigQuery table schema changes within the client application to prevent data loss or corruption due to mismatches.
    *   Utilize Google Cloud managed services and APIs where possible.
    *   Provide a cost-effective solution compared to the legacy `tabledata.insertAll` API.
    *   Ingest data with at-least-once semantics, with the capability to achieve exactly-once semantics through careful implementation of commit retries.
*   **Non-Goals:**
    *   Achieve sub-second data visibility in BigQuery (batch approach implies latency up to the batch interval + processing time).
    *   Implement a fully managed streaming pipeline using services like Dataflow or Pub/Sub (due to constraints).
    *   Automatically migrate the BigQuery table schema based on source data changes (detection is supported, migration requires a defined manual process).
    *   Provide a solution for ingesting data formats other than Protobuf via the Storage Write API in this specific implementation.

## ü¶â The Actual Design
---
The proposed design utilizes the BigQuery Storage Write API via the official `google-cloud-bigquery-storage` Python client library within the existing Python application running on Cloud Run. Data will be ingested in batches approximately every 10 minutes using the `Pending` stream type and Protocol Buffers (Protobuf) for serialization.

**Triggering Mechanism:**
The 10-minute batch interval is assumed to be triggered externally, likely via Cloud Scheduler invoking an HTTP endpoint on the Cloud Run service. This avoids issues with Cloud Run scaling to zero and ensures consistent batch timing. The specific implementation of the trigger is outside the scope of this RFC but is a prerequisite for deployment.

**Components:**
1.  **Python Application (Cloud Run):** The existing application will be modified to:
    *   Receive trigger (e.g., HTTP request from Cloud Scheduler).
    *   Buffer or collect data relevant to the ~10-minute interval.
    *   Define the data structure using a Protobuf schema (`.proto` file). This schema must be kept in sync with the BigQuery table schema. A process for versioning and deploying `.proto` files alongside application code is required. Note: As per documentation, the proto message cannot use a package specifier, and all nested/enum types must be defined within the top-level message if not using client library normalization; the Python client library handles this normalization.
    *   Serialize the buffered data batch into Protobuf format using code generated from the `.proto` file.
    *   Use the `google-cloud-bigquery-storage` Python library to interact with the BigQuery API.
    *   Implement robust error handling and logging.
2.  **BigQuery Storage Write API:** The application will interact with the API using the following flow for each batch:
    *   **`CreateWriteStream`:** Create a new stream of `Pending` type targeting the specific BigQuery table. This type buffers records on the service side until explicitly committed.
    *   **`AppendRows`:** Send the batch of Protobuf-serialized records to the created stream. The first `AppendRows` call on a stream must include the schema (as `DescriptorProto`). Subsequent calls append data. It's recommended to send the entire batch in one or a few large `AppendRows` calls for efficiency.
    *   **`FinalizeWriteStream`:** Mark the stream as finalized, indicating no more data will be appended to this specific stream instance. This must succeed before committing.
    *   **`BatchCommitWriteStreams`:** Atomically commit the stream(s). All data appended since stream creation becomes visible in BigQuery upon successful commit. This ensures the entire batch is ingested or none of it is, providing transactional guarantees. Careful retry logic for this call is needed if exactly-once semantics are required.
3.  **BigQuery Table:** The target table in BigQuery where data will be ingested. Table schema must align with the Protobuf schema used by the application. Configuration details (Project ID, Dataset ID, Table ID) should be securely managed.

**Data Flow Diagram:**

```mermaid
graph TD
    subgraph Trigger [External Trigger]
        Scheduler[Cloud Scheduler (or similar)]
    end

    subgraph CloudRunApp [Python Application (Cloud Run)]
        direction TB
        ReceiveTrigger["Receive Trigger"]
        Buffer["Buffer/Collect Data (~10 min)"]
        Serialize["Serialize Batch (Protobuf)"]
        APIClient["Storage Write API Client Logic"]
    end

    subgraph BQWriteAPI [BigQuery Storage Write API Interaction]
        direction TB
        CreateStream["1. CreateWriteStream (Pending)"]
        Append["2. AppendRows (Batch w/ Schema)"]
        Finalize["3. FinalizeWriteStream"]
        Commit["4. BatchCommitWriteStreams"]
    end

    subgraph ErrorHandling [Error Handling / Monitoring]
        LogError["Log Error Details"]
        Alert["Alerting (e.g., Cloud Monitoring)"]
        DLQ["Optional: Dead-Letter Queue (GCS)"]
    end

    BQTable[(BigQuery Table)]

    Scheduler -- Invokes --> ReceiveTrigger
    ReceiveTrigger --> Buffer
    Buffer -- Serializes --> Serialize
    Serialize -- Protobuf Batch --> APIClient

    APIClient -- Calls --> CreateStream
    CreateStream -- Success --> Append
    Append -- Success --> Finalize
    Finalize -- Success --> Commit
    Commit -- Success --> BQTable

    CreateStream -- Failure --> LogError
    Append -- Failure --> LogError
    Finalize -- Failure --> LogError
    Commit -- Failure --> LogError

    LogError --> Alert
    LogError -- Persistent Errors --> DLQ
```
*Diagram Note: The diagram illustrates the primary success path and basic error logging. A dead-letter queue (DLQ) is shown as optional but recommended for handling persistent errors.*

**Schema Evolution Handling:**
The Storage Write API can detect schema mismatches between the client's provided schema (`DescriptorProto` in the first `AppendRows` call) and the current BigQuery table schema.
1.  The application attempts the `AppendRows` call with its current Protobuf schema.
2.  If a schema mismatch error occurs (e.g., `StorageError` indicating schema mismatch), the API call fails.
3.  **Initial Strategy:** The application will log this persistent error clearly, including details of the mismatch if available, and trigger an alert for manual intervention. The batch will fail.
4.  **Manual Intervention Process:** An operator needs to:
    *   Investigate the schema mismatch.
    *   Update the BigQuery table schema if the change is intended and backward-compatible (e.g., adding nullable fields).
    *   Update the `.proto` file in the application's codebase, re-compile it, and deploy the updated application.
    *   Potentially re-process the failed batch if data loss is unacceptable (requires storing failed batches, e.g., in a DLQ).
5.  **Future Considerations:** A more automated approach (e.g., fetching the schema and attempting compatible transformations) is complex and out of scope for the initial implementation but could be considered later. The technical details of fetching/transforming schema dynamically require careful design. Non-backward-compatible changes will always require manual intervention and potentially code changes.

**Protobuf Usage:**
Using Protobuf offers advantages over JSON:
*   **Efficiency:** Binary format leads to smaller payloads and faster serialization/deserialization.
*   **Schema Enforcement:** Provides strong typing and explicit schema definition via `.proto` files.
The application requires a build step to compile `.proto` files into Python code. This compilation must be integrated into the CI/CD pipeline.

**Error Handling:**
Robust error handling is critical for reliability:
*   **Transient Errors:** Implement retries with exponential backoff and jitter for transient network or API errors during all Storage Write API calls (`CreateWriteStream`, `AppendRows`, `FinalizeWriteStream`, `BatchCommitWriteStreams`).
*   **Persistent Errors:**
    *   **Schema Mismatch:** Handled as described above (log, alert, fail batch).
    *   **Invalid Data (Poison Records):** Records causing persistent serialization or `AppendRows` errors (e.g., type mismatch not caught by Protobuf). The initial design implies the entire batch fails if `AppendRows` encounters an unrecoverable error for any record. **Recommendation:** Implement a dead-letter queue (DLQ) mechanism (e.g., writing failed records/batches to GCS) to prevent single bad records from halting ingestion. This allows valid data to proceed and failed data to be investigated separately.
    *   **Quota Exceeded (`429 Resource Exhausted`):** Monitor API quotas (concurrent connections, throughput). Implement appropriate backoff/retry strategies and potentially alert if quotas are consistently approached.
    *   **Commit Failures (`BatchCommitWriteStreams`):** If this call fails after `FinalizeWriteStream` succeeded, the stream is finalized but not committed. The client *must* retry the `BatchCommitWriteStreams` call with the same stream name(s). This operation is idempotent and crucial for achieving exactly-once semantics if required. Failure to retry or handle this state can lead to data loss for the batch.
*   **Logging:** Implement structured logging (e.g., JSON payload to Cloud Logging) with sufficient context (batch ID, timestamp, error details, relevant identifiers) to facilitate debugging.

**Simplicity:**
This design uses the `Pending` type, which aligns well with batch processing and provides atomicity per batch via `BatchCommitWriteStreams`. While the `Default Stream` offers a simpler API flow (no explicit finalize/commit), it provides only at-least-once semantics and lacks the atomic commit guarantee for the entire batch, making it less suitable for this use case where batch integrity is a goal. The added complexity of the `Pending` stream's commit phase is justified by the atomicity requirement.

## üåà Alternatives considered
---
|          | Option 1 (Proposed)                                  | Option 2                                        | Option 3                                      |
| -------- | ---------------------------------------------------- | ----------------------------------------------- | --------------------------------------------- |
| Overview | Storage Write API (`Pending` Stream, Protobuf)       | Storage Write API (`Default` Stream, Protobuf)  | Legacy `tabledata.insertAll` API (JSON)       |
| Links    | [Storage Write API](https://cloud.google.com/bigquery/docs/write-api) <br/> [Pending Type](https://cloud.google.com/bigquery/docs/write-api#pending_type) | [Default Stream](https://cloud.google.com/bigquery/docs/write-api#default_stream) | [Legacy Streaming](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-manipulation-language) (Implicitly via DML or older API docs) |
| Pros     | - Atomic batch commits (ACID per batch) <br/> - Efficient gRPC & Protobuf <br/> - Schema evolution detection <br/> - Lower cost than legacy API <br/> - Exactly-once possible (with careful commit retry) | - Simpler API flow (no finalize/commit) <br/> - Data available immediately for query <br/> - Efficient gRPC & Protobuf <br/> - Schema evolution detection <br/> - Lower cost than legacy API | - Simpler REST/JSON interface <br/> - Widely understood |
| Cons     | - More complex API flow (Create/Append/Finalize/Commit) <br/> - Data visible only after commit <br/> - Requires careful commit retry logic for exactly-once | - At-least-once semantics (potential duplicates on retry without client-side de-dupe logic) <br/> - No atomic commit for the whole batch | - Higher cost <br/> - Less efficient (REST/HTTP, JSON) <br/> - No transactional guarantees across rows <br/> - Stricter quota limits <br/> - No built-in schema evolution detection notification |
| Other    | Recommended for batch workloads needing atomicity.   | Recommended for streaming workloads needing low latency visibility. | Generally discouraged in favor of Storage Write API. |

**Comparison Summary & Rationale:**

*   **Legacy `tabledata.insertAll` API (Option 3):** Significantly more expensive, less efficient (REST/JSON), lacks transactional guarantees for batches, and has stricter quotas. Does not meet cost-effectiveness and reliability goals as well as the Storage Write API.
*   **Storage Write API - `Default` Stream (Option 2):** Simpler flow but provides at-least-once semantics (potential duplicates) and lacks atomic commits for the entire batch. Rows become visible immediately, which isn't required. Less suitable for the goal of reliable, atomic 10-minute batches.
*   **Storage Write API - `Pending` Stream (Option 1 - Proposed):** Explicitly designed for atomic batch commits using `BatchCommitWriteStreams`. Aligns perfectly with the requirement to ingest data reliably every 10 minutes as a single unit. Leverages efficiency (gRPC, Protobuf) and cost benefits. Supports schema evolution detection. The benefits of atomicity outweigh the slightly increased complexity, especially given the batch nature of the workload.

Therefore, the **Storage Write API with the `Pending` stream type and Protobuf** is the recommended approach.

## üí• Impact
---
*   **System Impact:**
    *   Requires modification of the existing Python application on Cloud Run (integrate library, implement batching, serialization, API logic).
    *   Introduces gRPC traffic between Cloud Run and the BigQuery Storage Write API endpoint. Ensure Cloud Run network configuration allows egress to Google APIs.
    *   BigQuery table will receive writes every ~10 minutes. Load testing is needed to ensure Cloud Run instance resources (CPU, memory) are sufficient for serialization and API calls under peak load.
*   **Operational Impact:**
    *   Requires deployment of the updated Python application, including the Protobuf compilation step in CI/CD.
    *   Requires monitoring of Cloud Run application logs and specific metrics (batch success/failure rate, processing latency, API error rates, quota usage). Dashboards and alerts need to be configured.
    *   Requires a documented process and potential on-call intervention for persistent errors (schema mismatches, poison records), especially if a DLQ is not implemented initially. Define alert severity and response procedures.
    *   Requires a documented process for managing schema evolution (updating BQ table, updating `.proto` file, deploying application).
*   **Cost Impact:**
    *   Primary cost driver is BigQuery Storage Write API ingestion volume. This is generally significantly lower than the legacy streaming API. A free tier exists (check current limits). An estimate based on expected data volume (average/peak) should be calculated.
    *   Cloud Run costs may slightly increase due to CPU/memory usage for processing and API calls.
    *   BigQuery storage costs apply as usual.
    *   Potential minor costs for Cloud Scheduler (trigger), Cloud Logging/Monitoring (depending on volume/metrics), and GCS (if DLQ is used).
    *   Recommend using GCP labels for cost tracking (e.g., on Cloud Run service, BigQuery dataset).
*   **Performance Impact:**
    *   End-to-end latency: ~10 minutes (batch interval) + application processing time + API call latency (`AppendRows`, `BatchCommitWriteStreams`). Data is visible only after successful commit.
    *   Throughput: Storage Write API generally offers high throughput (>= 1 MBps per connection, scalable). Potential bottlenecks include Cloud Run instance resources (CPU/memory for serialization) or network bandwidth for very large batches. Load testing is crucial.
    *   Define Service Level Objectives (SLOs) for ingestion success rate and latency (e.g., 99.9% of batches committed within 15 minutes).
*   **Security Impact:**
    *   **Authentication:** Cloud Run service account authenticates using Application Default Credentials (ADC). Ensure the service is invoked securely (e.g., via authenticated Cloud Scheduler, not public endpoint unless intended and secured).
    *   **Authorization (IAM):** The Cloud Run service account requires specific IAM permissions on the target BigQuery table. Grant the principle of least privilege, e.g., `roles/bigquery.dataEditor` contains `bigquery.tables.updateData`, but a custom role might be more appropriate if only insertion is needed. Provision via IaC.
    *   **Data Security:** Assess if the data contains sensitive information (PII, etc.). Implement necessary masking, tokenization, or encryption *before* serialization if required by compliance policies. Protobuf itself does not encrypt data at rest or in transit beyond standard gRPC TLS.
    *   **Secrets Management:** Use Secret Manager for any required secrets (e.g., configuration not suitable for env vars), not hardcoding. Ensure the Cloud Run service account has permission to access necessary secrets.
    *   **Network Security:** Consider deploying Cloud Run within a VPC Network and using VPC Service Controls to restrict data flow to BigQuery, enhancing protection against data exfiltration.
    *   **Audit Logging:** Ensure Cloud Audit Logs (Data Access logs for BigQuery, Admin Activity for Cloud Run) are enabled for monitoring and security investigations.

## üí¨ Discussion
---
*   **Data Volume:** What is the expected average and peak data volume (e.g., MB/GB, number of records) per 10-minute batch? *This is critical for cost estimation, performance tuning (Cloud Run resources), and ensuring quotas are sufficient.*
*   **Schema Evolution Strategy:** The initial plan is manual intervention upon detection. Is this operationally feasible given team availability and expected frequency of changes? *Documenting the manual process and potential impact (delay, data loss if reprocessing isn't possible) is essential.*
*   **Delivery Guarantee (Exactly-Once vs. At-Least-Once):** Is exactly-once delivery a strict requirement? The `Pending` type facilitates it but requires careful implementation and testing of `BatchCommitWriteStreams` retry logic. If at-least-once is acceptable, the implementation is slightly simpler, but downstream consumers might need de-duplication logic. *A clear decision is needed.*
*   **Poison Record Handling:** The default behavior (batch failure on persistent record error) poses an operational risk. Should a dead-letter queue (DLQ) mechanism be implemented in the initial version to improve resilience? *Strongly recommended to avoid blocking ingestion.*
*   **Trigger Mechanism:** Confirm Cloud Scheduler (or alternative) as the trigger and document its setup requirements.
*   **Protobuf Schema Management:** Define the process for versioning `.proto` files and ensuring synchronization between the application code and the BigQuery table schema during deployments.
*   **Python Client Library Behavior:** Confirm performance characteristics and schema normalization behavior of the `google-cloud-bigquery-storage` library under expected load, especially if using dynamic features.

## ü§ù Final decision
---
The final decision is to **adopt the BigQuery Storage Write API using the `Pending` stream type with Protocol Buffers (Protobuf)** for ingesting data batches from the Cloud Run Python application to BigQuery every 10 minutes.

**Justification:** This approach directly addresses the requirements for efficient, reliable, and atomic batch ingestion within the specified constraints (no Pub/Sub). It offers significant cost and performance advantages over the legacy streaming API and provides better transactional guarantees for batch workloads compared to the `Default` stream type. Schema evolution detection is supported, Protobuf ensures efficient data transfer, and the design leverages managed GCP services. The identified complexities (commit retries, schema management, error handling) are manageable with clear implementation and operational processes.

## ‚òùÔ∏è Follow-ups
---
*   **Implementation:**
    *   Define and version control the initial Protobuf schema (`.proto` file).
    *   Implement the batching, serialization, and Storage Write API (`Pending` type flow) logic in the Python Cloud Run application.
    *   Implement robust error handling:
        *   Retries with backoff for transient errors.
        *   Idempotent retry logic for `BatchCommitWriteStreams` (if exactly-once is required).
        *   Clear logging/alerting for persistent errors.
    *   Implement the chosen strategy for handling "poison records" (Recommendation: Implement DLQ to GCS).
    *   Integrate Protobuf compilation into the CI/CD pipeline.
    *   Implement structured logging.
*   **Configuration & Deployment:**
    *   Define and implement the trigger mechanism (e.g., Cloud Scheduler job).
    *   Configure secure management for BigQuery connection details (Project, Dataset, Table).
    *   Provision required IAM permissions for the Cloud Run service account using least privilege (via IaC).
    *   Estimate costs based on expected data volume and implement cost tracking labels.
*   **Operational Readiness:**
    *   Define and implement the specific strategy and *manual process* for handling schema evolution detection/errors.
    *   Define and implement the strategy for handling records sent to the DLQ (if implemented).
    *   Define specific monitoring metrics, SLOs, dashboards, and alerting thresholds (batch success rate, latency, API errors, quota usage).
    *   Conduct load testing to verify performance, resource requirements (Cloud Run), and quota limits under expected average and peak load.
    *   Create/update runbooks covering: deployment, monitoring, troubleshooting (transient errors, persistent errors, schema mismatch, DLQ handling), manual schema update process, on-call response procedures.
*   **Security & Compliance:**
    *   Review and configure network security (VPC-SC) if applicable.
    *   Review data sensitivity and implement necessary handling (masking, etc.).
    *   Review and configure secrets management.
    *   Ensure required Audit Logs are enabled.
    *   Document compliance adherence (data handling, residency).

## üîó Related
---
*   [BigQuery Storage Write API Documentation](https://cloud.google.com/bigquery/docs/write-api)
*   [Batch load data using the Storage Write API (Pending Type)](https://cloud.google.com/bigquery/docs/batch-load-storage-write-api)
*   [Python Client for BigQuery Storage API](https://cloud.google.com/python/docs/reference/bigquerystorage/latest)
*   [Protocol Buffer Basics (Python)](https://protobuf.dev/getting-started/pythontutorial/)
*   [BigQuery Storage Write API Quotas](https://cloud.google.com/bigquery/quotas#storage_write_api_limits)
*   [BigQuery Pricing - Data Ingestion](https://cloud.google.com/bigquery/pricing#data_ingestion_pricing)
*   [Cloud Run Documentation](https://cloud.google.com/run/docs)
*   [Cloud Scheduler Documentation](https://cloud.google.com/scheduler/docs)
*   [Google Cloud Monitoring](https://cloud.google.com/monitoring/docs)
*   [Google Cloud Logging](https://cloud.google.com/logging/docs)
